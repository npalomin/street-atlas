{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualising Data Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's workshop, we'll get to grips with how to use online APIs to generate greater insight from our raw datasets. The opening up of these APIs provides researchers and developers access to extremely useful tools to building context into data. The quality of the algorithms accessible through these interfaces often surpass what is possible using locally-installed packages.\n",
    "\n",
    "The workshop will draw on the skills you have built up over the last few weeks, and make use of the [**`requests`**](http://docs.python-requests.org) package to handle response files. We'll focus on using the Alchemy and Google APIs, but the skills you learn here should put you in a position to access and use most kinds of API in the future.\n",
    "\n",
    "**Important:** Before you start, you will have needed to have registered with Alchemy to recieve access to their APIs. You'll only be able to access these services with the API key that they provide you. For Alchemy, carry out the following steps:\n",
    "\n",
    "1. Sign up for Bluemix [here](https://console.ng.bluemix.net/registration/) and await the confirmation email.\n",
    "2. Login, navigate to *Services*, then to *Watson*, and click on *AlchemyAPI*.\n",
    "3. The AlchemyAPI page provides details about the service and options, read over and then navigate to *Create* near the bottom of the page. This will generate an **API key**, copy it from the Service Credentials section as you'll require later. \n",
    "4. (Optional) The default Bluemix account will only grant you 30 days free access - to bump this up to a year's access use [this](https://ibm.onthehub.com/WebStore/OfferingDetails.aspx?o=bb3528b7-2b63-e611-9420-b8ca3a5db7a1) link. A promo code will be sent to your UCL email, which you can use to top up your account within the Billing menu of your Bluemix account page.\n",
    "\n",
    "As usual, let's first **import the libraries** we'll need to start building deeper insight into our data. We'll also set a couple of display options to make life easier further down the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Requests and Alchemy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this first part is to get you acquainted with the `requests` library, and how it can be applied to the Alchemy API. \n",
    "\n",
    "**`Requests`** is a library for handling HTTP requests. These are requests which ask online APIs for a response, in the same way that you expect a response when you search http://urbanmovements.co.uk every single day (right guys?). However, instead of a website, the response will send back a JSON or XML file depending on what you have asked for (we'll focus on JSON here). The request string that you send to the API contains a load of options and authenticity parameters that narrow down your request. The `requests` library simplifies the process, yielding a file from which you can extract the API response.\n",
    "\n",
    "We'll also work with the **Alchemy API**. Alchemy provides a set of online tools that allow you to gain deeper insight into your text and image data. The text API tools will allow you to mine text for [sentiment](http://www.ibm.com/watson/developercloud/alchemy-language/api/v1/#sentiment) or [keywords](http://www.ibm.com/watson/developercloud/alchemy-language/api/v1/#keywords) or [language](http://www.ibm.com/watson/developercloud/alchemy-language/api/v1/#language). While the image tools will detect how many faces there are in a [picture](http://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#detect_faces) or [identify themes](http://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#classify_an_image). \n",
    "\n",
    "**Play around with these [here](http://www.alchemyapi.com/products/demo/alchemylanguage) and [here]( http://vision.alchemy.ai/) - to give you a sense of what they do.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's get started. The Alchemy API works by receiving request strings to an **endpoint**, and it's important that you understand how they work. We'll start work with the Keyword Extraction API, and find the keywords relating to a particular webpage. \n",
    "\n",
    "Go to the API docs [here](http://www.ibm.com/watson/developercloud/alchemy-language/api/v1/#keywords). This page contains all of the details you need to request and receive keywords relating to a webpage. **Read this page carefully, noting parameters that can be submitted, and the response formats provided in return.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now test for keywords on a specific webpage.** I've started building the query string below, but you'll need to complete it. We want to return the data in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The query has been broken down for you to make it clearer - add two parameters to make it work\n",
    "query = 'http://gateway-a.watsonplatform.net/calls/url/URLGetRankedKeywords?' \\\n",
    "        'url=http://www.thisnation.com/library/sotu/2009bo.html' \\\n",
    "        '&' \\\n",
    "        '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built the query, **run it using the script below**. This is the role of `requests`. It simply returns the data that you are asking for. Here we are telling it to expect a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(query).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now check out the data** using a simple print command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of a mess right? But, if you go back to the Alchemy API documentation, specifically around Response Format, you should be able to identify some structure.\n",
    "\n",
    "Next we'll put the returned data into a Pandas Dataframe. To do this, we'll use the pandas **`json_normalize`** function, which will extract the data in a useable format. **To load the library we use this command.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now go read the documentation on `json_normalize`, and then create a new dataframe which extracts all keywords from the JSON response. Remember the JSON structure is similar to that of a dict, so values can be extracted in the same way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = json_normalize(r['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a Dataframe suitable for combining with others, **check it out below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks good, but are we forgetting something? Have we forgotten what we learnt last week?! **Check that the data types are as you'd expect them to be, and if not, make necessary changes.** You'll need the data to be in the right format for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make type changes as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of tools lends itself to rolling out. Once we have the query string set up we only need to change one element of the string to run a completely different test.\n",
    "\n",
    "The most obvious change we can make in this case is to the webpage we test. The rest of the details can remain the same. **So now let's adjust the query up to receive a webpage defined in variable called `www`. You can use any website you like, but make sure it is similar to the one we used earlier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, set www\n",
    "www = \n",
    "# second, adjust your query above to include the www variable in place of the url parameter\n",
    "second_query = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run the new query, outputting the response into a Pandas Dataframe.** Make sure to use different variable names to those above, and remember to adjust the types where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two dataframe containing the keywords to two (hopefully) similar websites. The next task you have to to compare the two, and measure the differences in keyword prominence in each website. This will test your Pandas ninja skills, but also require you to do some reading of the Pandas docs.\n",
    "\n",
    "**As a first step, run a `.merge` command to create a new dataframe which joins the two dataframe on the keywords column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second, create a new column which calculates the difference in `relevance` metric for each keyword.** Remember to check your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, create a bar chart which shows the differences by keyword.** For this you'll need to consult the documentation on Visualisation [here](http://pandas.pydata.org/pandas-docs/stable/visualization.html). All you should need for this chart is one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've managed to create the chart above, **try again using a matplotlib style**, generating a more visually appealing output. The import statement below provides the import and style settings you need. This one will give you 'ggplot' styling, but **see if you can work out how to try another style too**. You can find more information on using these stylesheets [here](http://matplotlib.org/users/style_sheets.html).\n",
    "\n",
    "**Important:** You may need to install `matplotlib` through Package Manager, or upgrade your `matplotlib` package installation to above 1.5, for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - you've now managed to build some context into two websites, and run some basic analyses using Pandas. Well done!\n",
    "\n",
    "Now, let's experiment with a few more of the text APIs offered by Alchemy. Don't panic, in nearly all cases the format will be the same - you find the endpoint, send it the required parameters (including your authentication details) and handle the response. **In the spaces below, execute queries using the following APIs:**\n",
    "\n",
    "1. Sentiment Analysis \n",
    "\n",
    "2. Language Detection\n",
    "\n",
    "3. Face Detection \n",
    "\n",
    "If you're feeling adventurous, try using an API that requests data from something other than an URL. This could be sending text directly within the query string. If you want to send any other objects you'll need to URI encode it (e.g. make it suitable for inclusion within an HTTP request) using a library such as [`urilib`](https://docs.python.org/2/library/urllib.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Google Maps API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at how to use the Google Maps API. This API gives you access to some of the functionality provided by Google Maps - including extracting location data from addresses, and direction instructions between two points. Naturally, these are very useful sources of data, helping to add context through geographic association.\n",
    "\n",
    "The Google Maps API can be used in the same way as the Alchemy API - using the endpoint, we provide the parameters, request a JSON output, and add in our own authentication data. We then simply handle the JSON response using `requests` and `pandas`.\n",
    "\n",
    "**Important:** Before you start this part, you need to make sure you are signed up for a Google Maps API. To do this, follow [this link](https://console.developers.google.com/flows/enableapi?apiid=geocoding_backend&keyType=SERVER_SIDE&reusekey=true) to the Google Developers Console, hit 'Continue' to create a new project, and then 'Create' to create a key. You'll need a Google account to access these pages. You'll end up at a page containing a long-ish API code. Copy this for use in the next few queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Google Maps Geocoding API, first create the query string using the following code, inserting your own API key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = 'https://maps.googleapis.com/maps/api/geocode/json?' \\\n",
    "        'address=Warren Street London&' \\\n",
    "        'key=INSERT_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now execute the request using `requests`.** You can ignore the InsecurePlatformWarning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And, finally, put the results into a Pandas dataframe.** You'll see this yields us with all of the top level data contained within the JSON response. With this we can go ahead and use the location data how we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see it is almost exactly the same process as we used earlier. Which is great - a whole world of data APIs has been opened up to us!\n",
    "\n",
    "**Before moving on, have another play with the Geocoding API, or have a look at the Directions or Elevation APIs, working with the same work flow.** You may need to log back into the Google Developer Console to enable these services first, but the same API key will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Creating a Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we'll put together what we have learnt today with our burgeoning SQL and database skills. Here we'll develop a short script which replicates a workflow you might use in future to derive greater context from a dataset. In this case, we'll work with image data, using Alchemy to derive content tags from the raw images.\n",
    "\n",
    "The process will be as follows:\n",
    "\n",
    "1. Extract existing data from your database\n",
    "2. Iterate over dataset, running query on each data point\n",
    "3. Export findings back to your database\n",
    "\n",
    "Using this process you'll have a new set of contextual data with which to undertake deeper data analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get started, run the following commands to establish a connection to your database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the SQLAlchemy libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# create the connection string to the MySQL database\n",
    "# replace USERNAME and PASSWORD with your own credentials \n",
    "engine = create_engine('mysql+pymysql://USERNAME:PASSWORD@128.40.150.34:3306/USERNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study we'll work with the Flickr data we dealt with in the first week. If you remember, this data consisted of a number of tables, including one which contained the original URL for each photograph. While the data contained user-specified tags, using the [Image Tagging API](http://www.alchemyapi.com/api/image-tagging/urls.html), Alchemy can provide a more objective view on the contents of the photograph.\n",
    "\n",
    "**Use the `.read_sql` command you used last week to create a new dataframe containing both `pid` and `download_url` from the `photos` table.** Limit your results to only 50 photographs for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the contents of your new dataframe to make sure everything came through fine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the short script that will loop through our dataframe, run the Image Tagging analysis (check the docs!), and write our data back to the database. You've done most of this before, but I've provided a few pointer bits of script to help you along your way.\n",
    "\n",
    "**Write the script, and check the results have gone through to your database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate over photos dataframe\n",
    "for index, row in photos.iterrows():\n",
    "    \n",
    "    # create query string with placeholder for url\n",
    "    \n",
    "    # execute request on url\n",
    "    \n",
    "    # put output into Pandas dataframe\n",
    "    \n",
    "    # add column to the dataframe containing the photo id (important for linking the data in future)\n",
    "    \n",
    "    # (optional) add a print command to give yourself a progress update\n",
    "\n",
    "    # if text (in the first row) does not equal 'NO_TAGS', upload to database\n",
    "    if ... :\n",
    "        # now upload to the database - this will create a new 'photo_tags' table for us, and we'll choose to not upload the index.\n",
    "        df.to_sql('photo_tags', conn, flavor='mysql', if_exists='append', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great - you've now created a workflow - taking existing data, adding context, and recording it in a database for further analysis!**\n",
    "\n",
    "Next week we'll look at some of the computational tools we can use to derive further insight from our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
